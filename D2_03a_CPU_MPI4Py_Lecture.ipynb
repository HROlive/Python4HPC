{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa07082-3880-49f6-a298-b873d75a7d28",
   "metadata": {},
   "source": [
    "# MPI4Py - Message Passing Interface for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bb6c8-0202-4e97-b01d-de2ad76bac4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e433225-b970-4b75-93cf-246e43dc6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/fstmpi.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD # communicator size\n",
    "rank = comm.Get_rank() # process rank\n",
    "sndbuf = np.array([rank]) # send buffer\n",
    "rcvbuf = np.empty_like(sndbuf) # receive buffer\n",
    "\n",
    "comm.Reduce([sndbuf, 1, MPI.INT], [rcvbuf, 1, MPI.INT], op=MPI.SUM, root=0) # sum reduction on process 0 (root)\n",
    "\n",
    "print(f'Process {rank}: Sending {rank} to 0.')\n",
    "if rank == 0:\n",
    "    print(f'Root: Sum of ranks is {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee544fd6-37d3-441d-968c-4e0dfa266b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mpirun --oversubscribe --np 8 python3 mpi4py/fstmpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69bdc0-d303-42e6-9a04-d6fdef7255cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c876946-133e-47c7-91be-47d49537ee5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Processes & Communicators\n",
    "MPI programs follow the __SPMD__ (Single program, multiple data) programming paradigm, where each process runs the same executable but potentially with different data. In the pure (single-threaded) MPI model, each core on a node can execute an __MPI process__, even though there are most certainly multiple cores in each physical processor and perhaps even multiple processors on each node. These processes communicate via explicit __message passing__ over a __transparent network__, such that in general the programmer need not care whether the communicating processes are located on the same processors or distributed over several nodes.\n",
    "MPI processes are organized in __logical sets__ that define which processes are allowed to communicate with each other. Such a set of processes is known as a __communicator__. One special communicator that contains all processes is created at the start of an MPI program; this communicator is called __MPI.COMM_WORLD__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd2faa-6028-4fcb-8b2a-60453bdf9f8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pickling\n",
    "__Pickling__ refers to the serialization and deserealization of Python objects, which is commonly done by the help of the `pickle` module. Before objects can be stored or transferred via network, they need to be converted into a byte stream that preserves the objects' structure. The inverse process converts this byte stream back into an object that is identical to the original. __MPI4Py__ provides pickle-based communication of generic Python object as well as direct array data communication of buffer-provider objects, such as NumPy arrays. Communication functions with all-lowercase names are meant for generic pickled objects, while those starting with an __upper-case__ letter are used for __buffered objects__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0e5a8-58d7-4fee-90d1-9f7e180f466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Bob.py\n",
    "\n",
    "class Bob:\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n",
    "\n",
    "    def report(self, a, b, c):\n",
    "        return f'{a*b + c} {self.msg}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa66da-a1e9-44f7-9696-7db62937166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Bob import *\n",
    "\n",
    "\n",
    "bobj = Bob('bottles of beer on the wall') # creates a Bob object\n",
    "print(bobj.report(4,20,19)) # calls Bob function\n",
    "\n",
    "with open('bobfile.pkl', 'wb') as picklefile: # creates a pickle file (write binary mode)\n",
    "    pickle.dump(bobj, picklefile) # pickles the Bob object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d48da-a109-41bb-b55c-0a770a11bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('bobfile.pkl', 'rb') as file: # read the pickle file (read binary mode)\n",
    "    bobj = pickle.load(file) # unpickles the Bob object\n",
    "\n",
    "print(bobj.report(4,21,16)) # calls Bob function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d7b36-1a60-45b7-b2de-e658968e6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile picklempi.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD # world communicator size\n",
    "rank = comm.Get_rank() # process rank\n",
    "size = comm.Get_size() # communicator size\n",
    "\n",
    "if rank == 0:\n",
    "    with open('bobfile.pkl', 'rb') as file: # read the pickle file (read binary mode)\n",
    "        bobj = pickle.load(file) # unpickles the Bob object\n",
    "else:\n",
    "    bobj = None\n",
    "\n",
    "bobj = comm.bcast(bobj, root=0) # root broadcasts the Bob object\n",
    "print(bobj.report(size,size,rank)) # calls Bob function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c07517-3776-4578-a79c-df74796d30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -export-bindings --bind-to-core --np 8 python3 picklempi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b99e3-ef21-402f-8889-ab1ae089cd46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Point-to-Point Communication\n",
    "\n",
    "The simplest method to communicate with MPI is __point-to-point communication__ between two specific processes, a sender and a receiver. Both processes actively participate in this form of communication where the sender must execute some send function while the receiver executes some receive function. Furthermore, both processes must have the following information: the communication patner (i.e. source or destination) and the tag that identifies the message.\n",
    "MPI is equipped with two flavors of point-to-point communication: blocking and non-blocking.\n",
    "With blocking communication, the processes wait until the communication has reached a certain state before they continue to process the data, while processes engaging in non-blocking communication continue immediately and require the programmer to check whether it is safe to process the data.\n",
    "\n",
    "| SENDING | Blocking | Nonblocking |\n",
    "|---|---|---|\n",
    "| Synchronous | `Ssend` | `Issend` |\n",
    "| Buffered  | `Bsend` | `Ibsend` |\n",
    "| Standard  | `Send` | `Isend` |\n",
    "| Ready  | `Rsend` | `Irsend` |\n",
    "\n",
    "| RECEIVING | Blocking | Nonblocking |\n",
    "|---|---|---|\n",
    "| Standard  | `Recv` | `Irecv` |\n",
    "\n",
    "| COMPLETING | Blocking | Nonblocking |\n",
    "|---|---|---|\n",
    "| Standard | `Wait` | `Test` |\n",
    "| Any  | `Waitany` | `Testany` |\n",
    "| Some  | `Waitsome` | `Testsome` |\n",
    "| All  | `Waitall` | `Testall` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a8e4f-f9a0-4fec-9b3c-c1093cfc764f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Blocking Point-to-Point Communication\n",
    "\n",
    "Blocking send or receive functions cause the executing process to suspend until the message buffer is safe to use. After a blocking send, the process only continues when the data to be sent have been copied from the send buffer, however, this does not mean that the data have been received. In the case of a blocking receive, the completion impies that the data have been copied to the receive buffer and is safe to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0088158-e11e-4c33-ab49-1d9149b3ce4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo-bp2p.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    comm.Send(sndbuf, dest=1) # standard blocking send\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a964b7-11ce-42f8-93ba-9e352ff235e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo-bp2p.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341e2a4-edfc-4990-9b62-d269650cff0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Communication Modes\n",
    "\n",
    "For blocking point-to-point communication, the MPI standard defines four modes of communication with subtle differences in their semantics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8e4a2-c394-408c-ad29-08f42778e516",
   "metadata": {},
   "source": [
    "__Synchronous Send__ is the most stringent communication mode, since the sending process requires the receiving process to provide a matching receive, i.e. it has to accept the handshake, in order to initiate the send. This means that the receiving process has to declare its readinoss for receiving a message. Ideally, every MPI program still works correctly when standard send is replace with synchronous send, however, if it is used incorrectly, it can lead to deadlocks and serialization. The standard use case for this mode is debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5eada-e048-4f32-9f5f-f0b7f16ac162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_bp2p-synchronous.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    comm.Ssend(sndbuf, dest=1) # synchronous blocking send\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924f76f-1852-46da-a915-b81a239d5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo_bp2p-synchronous.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0602c-5322-4e07-80ce-3baa54514bed",
   "metadata": {},
   "source": [
    "__Buffered Send__  copies the data from the message buffer to a buffer that is managed by the user and subsequently returns. Once a matching receive has been received, the data will be transmitted over the network from the user's buffer. Naturally, this requires an additional buffer and an extra transfer between the buffers. However, this communication mode is local, and its completion does not depend on the occurrence of a matching receive. This communciation mode also requires the programmer to attach and detach a user-managed buffer, where the detach call blocks und all messages in the buffer have been transmitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad75c4-1966-428e-ac3c-8bdffb47dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_bp2p-buffered.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    MPI.Attach_buffer(sndbuf) # attach buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    comm.Bsend(sndbuf, dest=1) # buffered blocking send\n",
    "    MPI.Detach_buffer() # detach buffer\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47907fbe-8254-4b22-acf7-5eae0b7cead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo_bp2p-buffered.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171323c-4299-48f1-92c3-c2d487b80b74",
   "metadata": {},
   "source": [
    "__Standard Mode__ is either synchronous or bufferd, depending on the MPI library, and comes with the respective advantages and disadvantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f151a-6987-4c35-a1cb-596d875ea91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_bp2p-standard.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    comm.Send(sndbuf, dest=1) # standard blocking send\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fecea-472d-4635-ad17-317349f90485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo_bp2p-standard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf342fa-1906-4513-95c4-2efdb086683a",
   "metadata": {},
   "source": [
    "⚡ __Ready Send__ ⚡ communication works under the assumption that the matching receive has alread been posten and thus the send call completes immediately. However, this call only succeeds if the matching receive has indeed been posted, otherwise the behvaiour is undefined. This communication has the potential to be that fastest but it should be handled with utmost care and used only when the control flow of the parallel program permits it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145dc61-0fcb-4aad-a976-d1b4958b8b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_bp2p-ready.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    comm.Rsend(sndbuf, dest=1) # ready blocking send\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f0ca8-4898-4bcd-bee9-b43c504f226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo_bp2p-ready.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b79e7-b9ac-4428-9198-b5735a94df9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nonblocking Point-to-Point Communication\n",
    "\n",
    "Nonblocking calls only initiate send or receive operations but do not complete them. The calls will return before the message has been copied to or from the buffer and a separate call is necessary to complete the operation. It is the programmers responsibility to leave the buffer unmodified between initiation and completion of a call. Only after calling test or wait functions the buffers can be savely read or written. The function names of these nonblocking send and receives calls irrespective of their communstart with a capital I, which stands for immediate\n",
    "The primary reason for introducing nonblocking communication into a program is to overlap computation and communication by offloading the communication part to the network hardware with minimal involvement of the CPU, which can continue with the computation part in the meantime.\n",
    "MPI provides nonblocking alternatives to all the previously mentioned communication modes for sending and receiving as well as a set of functions to check for completion of transmission.\n",
    "\n",
    "* Blocking sends can be used with nonblocking receives and vice versa\n",
    "* Nonblocking calls followed immediately by a matching wait are equivalent to blocking calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed49a0-81f3-4258-a7f2-666bc3e3774e",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "`MPI.Test` returns immediately with a flag that indicates whether the given request is completed. Blocking behaviour can be emulated with calling `MPI_Test` inside a loop, which turns it into a safe polling mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4da07-8f37-4897-92a6-fc981e498640",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_ip2p-standard-test.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    req = comm.Isend(sndbuf, dest=1) # standard blocking send\n",
    "    if (req.Test() is not True):\n",
    "        print(f'Isend of process {rank} pending ...')\n",
    "    if (req.Test() is True):\n",
    "        print(f'Isend of process {rank} successful.')\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    req = comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70c65a-098b-4793-8f86-f79658483821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo_ip2p-standard-test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5187ff3-af78-4dd0-a7a5-4d75e6c089f7",
   "metadata": {},
   "source": [
    "#### Wait\n",
    "\n",
    "`MPI.Wait` is essentially the blocking version of `MPI.Test` that returns only when the operation corresponding to the given request has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a116a-953f-4a76-a7d4-fda51d61f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_ip2p-standard-wait.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    req = comm.Isend(sndbuf, dest=1) # standard blocking send\n",
    "    if (req.Wait() is True): # instance method\n",
    "        print(f'Isend successfull')\n",
    "elif rank == 1:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    comm.Recv(rcvbuf, source=0) # blocking receive\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3daefe-54ff-4c5d-8237-6ae1bf272689",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --bind-to core --np 2 python3 mpi4py/demo_ip2p-standard-wait.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd8676-f7f1-4a5a-892b-b4ec1f9ab374",
   "metadata": {},
   "source": [
    "#### Testany\n",
    "\n",
    "`MPI.Testany` and its friends can be used when the order of completed requests is irrelevant. This call returns immediately and indicates whether any one request has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc637a3-100d-4789-905d-dcca85728362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_ip2p-standard-testany.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "reqs = [] # requests\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    for i in range(1, comm.Get_size()):\n",
    "        req = comm.Isend(sndbuf, dest=i) # standard nonblocking send\n",
    "        reqs.append(req)\n",
    "    MPI.Request.Testany(reqs) # class method\n",
    "elif 0 < rank:\n",
    "    rcvbuf = np.empty_like(data) # receive buffer\n",
    "    req = comm.Irecv(rcvbuf, source=0) # nonblocking receive\n",
    "    # print(f'Process {rank} receives {rcvbuf}') # unsafe!\n",
    "    req.Wait() # instance method\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913673d-888f-4f4e-8723-9dd9637d2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --oversubscribe --np 8 python3 mpi4py/demo_ip2p-standard-testany.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d03ad1-6fd6-4b32-9935-82eb7190fedf",
   "metadata": {},
   "source": [
    "#### Waitall\n",
    "\n",
    "`MPI.Waitall` blocks until all given requests have been completed and has similar relatives as `MPI.Testany`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93019d7-b2bb-42c1-86b1-c3e03165fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_ip2p-standard-waitall.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = [0.12, 3.45, 6.78, 9.10]\n",
    "reqs = []\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.array(data) # send buffer\n",
    "    print(f'Process {rank} sends {sndbuf}')\n",
    "    for i in range(1, comm.Get_size()):\n",
    "        req = comm.Isend(sndbuf, dest=i) # standard nonblocking send\n",
    "        reqs.append(req)\n",
    "    print(f'Process {rank} waits for all requests to complete')\n",
    "    MPI.Request.Waitall(reqs) # class method\n",
    "    print(f'Process {rank} finished waiting')\n",
    "elif 0 < rank:\n",
    "    rcvbuf = np.zeros_like(data) # receive buffer\n",
    "    req = comm.Irecv(rcvbuf, source=0) # blocking receive\n",
    "    # print(f'Process {rank} receives {rcvbuf}') # unsafe!\n",
    "    req.Wait()\n",
    "    print(f'Process {rank} receives {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b4083-c112-411b-aaad-87e5f38826dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --oversubscribe --np 8 python3 mpi4py/demo_ip2p-standard-waitall.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca7c41-3d4f-4972-a5ca-50d46fe7b4b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collective Communication\n",
    "\n",
    "\n",
    "So far, processes have communicated directly with each other and without any involvement of other processes. In collective communication, on the other hand, all processes in a communicator are involved either by sending messages directly to each other or by forwarding messages. Usually, the motivation behind using this mode of communication is to manipulate a shared set of information, e.g. a problem that requires distribution over several compute nodes due to its size. This collective communication routines are internally and transparently built upon point-to-point communication functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce936886-dce3-4a6d-b34a-ce933c9c4756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Communicators\n",
    "\n",
    "Communicators are the centerpiece of collective communication; all collective communication happens relative to them. For all intents and purposes, communicators consist of two parts: a groupd and a context. The context helps to distinguish messages within a communicator from those in other communicators and allows a process to be in several communicators at once. Messages sent in one context cannot be received in another context. A group is nothing else than the group of processes with in a communicator. While the context is generally transparent to the user, groups are convenient for efficiently creating new communicators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c69ed0-a9c9-4f62-b4cf-f720e295fde1",
   "metadata": {},
   "source": [
    "#### Splitting\n",
    "\n",
    "A common way to create a new communicator is to __split__ the old one and separate its processes depending on their rank. To this end, each process is assigned a `color` and all processes with the same color end up in a common communicator. Additionally, the processes' ranks in the new communicator are ordered depending on the `key` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c693b-4c5f-4525-8263-b087e1d20f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_split.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(f'Process {rank} in comm({size})')\n",
    "\n",
    "newcomm = comm.Split(rank % 2, key=rank)\n",
    "newrank = newcomm.Get_rank()\n",
    "newsize = newcomm.Get_size()\n",
    "\n",
    "print(f'Process {rank} in comm({size}) \\t {newrank} in newcomm({newsize})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25be8ed-24ed-4eb6-a690-544dbf6677e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/comm_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2e315-14c6-46a4-8206-b4a877679012",
   "metadata": {},
   "source": [
    "#### Groups\n",
    "\n",
    "The main difference from communicators is that groups do not enable communication between processes, instead, they provide local routines to build new groups via set operations and new communicators can be established from these groups. This implies that set operations are local and the creation of a new communicator from a group is collective only over the processes within that group. Communicators are essentially groups with the additional ability to communicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d7c82-0779-473b-b83f-fe31a66d292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_groups.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "grpworld = comm.Get_group() # local\n",
    "grpeven = grpworld.Incl([0, 2, 4, 6]) # local\n",
    "grpodd = MPI.Group.Difference(grpworld, grpeven) # local\n",
    "\n",
    "commeven = comm.Create_group(grpeven) # collective only over grpeven\n",
    "commodd = comm.Create_group(grpodd) # collective only over grpodd\n",
    "\n",
    "if (commeven != MPI.COMM_NULL):\n",
    "    print(f'Process {rank} is in commeven')\n",
    "    \n",
    "if (commodd != MPI.COMM_NULL):\n",
    "    print(f'Process {rank} is in commodd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ecee0-8461-474f-bcc8-b46bc6b7b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_groups.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736fbcf-6f6c-4d19-8212-dececb5aee2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collective Communication\n",
    "\n",
    "Of course there are more interesting things to do than splitting process groups, for example, moving data within a group of processes instead of only sending and receiving from two specific processes. These operations are where MPI can really play its strengths. To this end, MPI provides __three types__ of collective data-movement routines: broadcast, gather, and scatter. In each of which, a process either sends to or receives a __fixed amount of data__ from all processes. For gathering and scattering there are also versions that support a __variable amount of data__ for each process and whose function name is suffixed with a lowercase V. Moreover, since MPI-3 __nonblocking collective communication__ functions are also available. Similar to the nonblocking point-to-point routines, the function names start with a capital I and return a `request` object that can be passed to wait and test functions.\n",
    "\n",
    "__NOTES__\n",
    "* collectives need to be called by all processes in a communicator\n",
    "* amount of data must be known\n",
    "\n",
    "\n",
    "\n",
    "| FIXED-DATA | Blocking | Nonblocking |\n",
    "|---|---|---|\n",
    "| Broadcast | [`Bcast`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Bcast) | [`Ibcast`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Ibcast) |\n",
    "| Scatter  | [`Scatter`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Scatter) | [`Iscatter`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Iscatter) |\n",
    "| Gather  | [`Gather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Gather) | [`Igather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Igather) |\n",
    "| Allgather  | [`Allgather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Allgather) | [`Iallgather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Iallgather) |\n",
    "| Alltoall  | [`Alltoall`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Alltoall) | [`Ialltoall`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Ialltoall) |\n",
    "\n",
    "| VARIABLE-DATA | Blocking | Nonblocking |\n",
    "|---|---|---|\n",
    "| Broadcast | - | - |\n",
    "| Scatter  | [`Scatterv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Scatterv) | [`Iscatterv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Iscatterv) |\n",
    "| Gather  | [`Gatherv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Gatherv) | [`Igatherv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Igatherv) |\n",
    "| Allgather  | [`Allgatherv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Allgatherv) | [`Iallgatherv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Iallgatherv) |\n",
    "| Alltoall  | [`Alltoallv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Alltoallv) | [`Ialltoallv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Ialltoallv) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e0280-271d-4f00-a54f-c754507bd5fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Broadcast\n",
    "\n",
    "A broadcast is used when one distinguished process, often called the \"root\", sends the same data to all processes in a communicator.\n",
    "__one-to-all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36bbf16-578f-49e4-af2e-815fa80eb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_broadcast.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.array([2.71, 8.28, 1.82, 8.459], dtype=np.float32)\n",
    "else:\n",
    "    data = np.empty(4, dtype=np.float32)\n",
    "\n",
    "print(f'Process {rank} initially has {data}')\n",
    "if rank == 0: print(f'Process {rank} broadcasts {data}')\n",
    "comm.Bcast(data, root=0) # blocking broadcast\n",
    "print(f'Process {rank} received {data} from process 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519bf50-003e-4ec9-8258-751187bbf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 4 --oversubscribe python3 mpi4py/demo_broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582261c-b8f2-46ff-a1b0-1d064cc6157b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scatter\n",
    "\n",
    "While a broadcast operation distributes the same data from the root process to all other processes, a scattering operation sends different data from the root to every process. __one-to-all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fafac-7505-41cc-a01b-66c2eac5b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_scatter.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "sndbuf = None # send buffer\n",
    "rcvbuf = np.empty(1, dtype=np.int32) # receive buffer\n",
    "\n",
    "if rank == 0:\n",
    "    sndbuf = np.arange(start=0, stop=comm.Get_size(), step=1, dtype=np.int32)**2\n",
    "    print(f'Process {rank} scatters {sndbuf}')\n",
    "\n",
    "comm.Scatter(sndbuf, rcvbuf, root=0) # blocking broadcast\n",
    "print(f'Process {rank} received {rcvbuf} from process 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8dacb-707a-4248-bbe0-9670f0971de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5a220-2412-438b-b1e3-99dded1eca41",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gather\n",
    "\n",
    "The gather operation allows a distinguishd process to collect specific array elements from each process. This is the inverse of the scattering operation. __all-to-one__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0b08c-c771-435e-98f0-c8e8dae6e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_gather.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "sndbuf = sndbuf = np.array([rank, rank], dtype=np.int32)**2 # send buffer\n",
    "rcvbuf = None # receive buffer\n",
    "\n",
    "if rank == 0:\n",
    "    rcvbuf = np.empty(2*comm.Get_size(), dtype=np.int32) # receive buffer\n",
    "\n",
    "print(f'Process {rank} sends {sndbuf}')\n",
    "comm.Gather(sndbuf, rcvbuf, root=0) # blocking gather\n",
    "\n",
    "if rank == 0:\n",
    "    print(f'Process {rank} gathered {rcvbuf} from processes 0 to {comm.Get_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bf359-2303-44a1-ba37-9c57a7cbbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d3a1b-e8d0-49d1-b524-07862a6253be",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Allgather\n",
    "\n",
    "Allgather operations can be understood as a gather operation with the addition of all processes receiving the result, instead of only the root. Practically, this is equivalent to a gather operation followed by a broadcast operation by the gathering process, however, a respectable MPI library uses a specialized algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ffc00-a271-4e08-93da-4b7155ce9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_allgather.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "sndbuf = sndbuf = np.array([rank, rank], dtype=np.int32)**2 # send buffer\n",
    "rcvbuf = np.empty(2*comm.Get_size(), dtype=np.int32) # receive buffer\n",
    "\n",
    "print(f'Process {rank} sends {sndbuf}')\n",
    "comm.Allgather(sndbuf, rcvbuf) # blocking allgather\n",
    "print(f'Process {rank} allgathered {rcvbuf} from processes 0 to {comm.Get_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6317eb6-c3b3-4a4d-8baa-1cbe9cdf77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_allgather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1d498-e43f-44d4-a4c0-c5935dc27df4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Allgatherv\n",
    "\n",
    "This is simply the variable-data version of an allgather operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f06e4-3346-45b9-9839-c50c83d7c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_allgatherv.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "sndbuf = np.array([0] if (rank == 0) else [rank]*rank, dtype=np.int32) # send buffer\n",
    "rcvbuf = np.empty(7, dtype=np.int32) # receive buffer\n",
    "counts = [1, 1, 2, 3]\n",
    "\n",
    "print(f'Process {rank} sends {sndbuf}')\n",
    "comm.Allgatherv(sndbuf, [rcvbuf, [1, 1, 2, 3], MPI.INT]) # blocking allgather\n",
    "print(f'Process {rank} allgatherved {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d6a29-0dd0-4523-9621-45a51eb017ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 4 --oversubscribe python3 mpi4py/demo_allgatherv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7382b16-b0a0-463b-9378-a8d3153e282c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iscatterv\n",
    "\n",
    "Unsurprisingly, there is also a nonblocking variable-data version of the scatter operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1fc3f-1a84-4707-8787-73f190d82c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_iscatterv.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "sndbuf = None\n",
    "rcvbuf = np.empty(1 if (rank == 0) else rank*rank, dtype=np.int32)\n",
    "\n",
    "if (rank == 0):\n",
    "    sndbuf = np.array([0] if (rank == 0) else [rank]*rank, dtype=np.int32) # send buffer\n",
    "    \n",
    "print(f'Process {rank} sends {sndbuf}')\n",
    "comm.Iscatterv([sndbuf, [rcvbuf, [1, 1, 2, 3], MPI.INT], MPI.INT], rcvbuf, root=0) # blocking allgather\n",
    "print(f'Process {rank} iscatterved {rcvbuf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d31c9-b260-40e9-a7d2-cf5282355d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 4 --oversubscribe python3 mpi4py/demo_iscatterv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b819c-58ae-403d-9ca4-bdacaf153a40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collective Computation\n",
    "\n",
    "Additionally to moving data around between processes, MPI can also perform basic computations on distributed data. This is implemented in the form of __reduce__ and __scan__ operations, where the former returns only the complete result and the latter returns incremental results on each process. The operation to be performed is given to the routine as an argument, which can either be one of the __built-in operations__, e.g. summation or finding a maximum value, or a __user-defined operation__. The standard reduce function is a so-called __rooted collective__, since the result of the reduction is only available on a distinguished root process, however, there is also a __non-rooted__ version, where the result is available on all processes, and a __non-blocking__ version available.\n",
    "\n",
    "__Disclaimer:__ As of August 2022, MPI4Py unfortunately does not support MPI's scan or exscan routines.\n",
    "\n",
    "| FIXED-DATA | Blocking | Nonblocking |\n",
    "|---|---|---|\n",
    "| Reduce | [`Reduce`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Bcast) | [`Ireduce`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Ibcast) |\n",
    "| Allreduce  | [`Allreduce`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Scatter) | [`Iallreduce`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Iscatter) |\n",
    "| Scan  | - | - |\n",
    "| Exscan  | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef928493-8cdc-41e7-af68-85e39ff333dd",
   "metadata": {},
   "source": [
    "#### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f7d3ef-77de-443c-ad5a-f3fd1ac6cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mpi4py/demo_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi4py/demo_reduce.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "rcvbuf = np.zeros(2, dtype=np.int32)\n",
    "sndbuf = np.array([rank, size], dtype=np.int32)\n",
    "\n",
    "print(f'Process {rank} sends {sndbuf} via Reduce')\n",
    "comm.Reduce(sndbuf, rcvbuf, root=0, op=MPI.SUM)\n",
    "print(f'Process {rank} receives {rcvbuf} via Reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ac8648-dfc7-46ee-a71c-dd421afcaaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 7 sends [7 8] via Reduce\n",
      "Process 0 sends [0 8] via Reduce\n",
      "Process 4 sends [4 8] via Reduce\n",
      "Process 5 sends [5 8] via Reduce\n",
      "Process 5 receives [0 0] via Reduce\n",
      "Process 7 receives [0 0] via Reduce\n",
      "Process 1 sends [1 8] via Reduce\n",
      "Process 3 sends [3 8] via Reduce\n",
      "Process 1 receives [0 0] via Reduce\n",
      "Process 3 receives [0 0] via Reduce\n",
      "Process 6 sends [6 8] via Reduce\n",
      "Process 6 receives [0 0] via Reduce\n",
      "Process 2 sends [2 8] via Reduce\n",
      "Process 2 receives [0 0] via Reduce\n",
      "Process 0 receives [28 64] via Reduce\n",
      "Process 4 receives [0 0] via Reduce\n"
     ]
    }
   ],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb522b3-7ab0-4e80-869b-e2ac34f5ef54",
   "metadata": {},
   "source": [
    "#### Allreduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca1a03-8fca-4182-89bc-3a16c2cc2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_allreduce.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "rcvbuf = np.zeros(2, dtype=np.float32)\n",
    "sndbuf = np.array([rank, size], dtype=np.float32)\n",
    "\n",
    "print(f'Process {rank} sends {sndbuf} via Reduce')\n",
    "comm.Allreduce(sndbuf, rcvbuf, op=MPI.PROD)\n",
    "print(f'Process {rank} receives {rcvbuf} via Reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cf003-2d70-48e4-80e8-2a11b829a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_allreduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7bf1d-6f02-44d8-8393-72998f82b62c",
   "metadata": {},
   "source": [
    "#### Iallreduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a6bb7-b9e4-4077-8baf-3020d6ad4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_iallreduce.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "rcvbuf = np.zeros(2, dtype=np.float32)\n",
    "sndbuf = np.array([rank, size], dtype=np.float32)\n",
    "\n",
    "print(f'Process {rank} sends {sndbuf} via Reduce')\n",
    "req = comm.Iallreduce(sndbuf, rcvbuf, op=MPI.SUM)\n",
    "# some local computation\n",
    "req.Wait()\n",
    "print(f'Process {rank} receives {rcvbuf} via Reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78b886-cf57-466b-90ce-229c818647f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_iallreduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca55cd-853f-4482-becc-7bceb4506b77",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collective Synchronization\n",
    "\n",
    "With the barrier, MPI provides a single global synchronisation operation. A process that posts a call to this function halts until all other processes in the communicator have also done so. It is tempting to understand this as some kind of checkpoint from which all processes proceed at the same time, but this is not the case. The call to the barrier blocks until all processes have reached the barrier, afterwards the processes are free to proceed. Barriers are useful only for a handful of cases, however, they are not necessary in an ideal program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e8d6c-1a9f-4edf-9555-c98e8eb665d1",
   "metadata": {},
   "source": [
    "#### Barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828b731-c562-409f-af1d-1de6ab187812",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_barrier.py\n",
    "\n",
    "import time\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "time.sleep(float(rank)/2)\n",
    "print(f'Process {rank} has reached the barrier')\n",
    "comm.Barrier()\n",
    "print(f'Process {rank} has passed the barrier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f609f3-321f-43ea-a553-74214a531e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 8 --oversubscribe python3 mpi4py/demo_barrier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ced6b1-f1ed-4b32-a09a-79b079c520de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## One-sided Communication\n",
    "\n",
    "Collective as well as point-to-point communication share the characteristic that in general two processes have to actively participate in the communication, a sender and a receiver. More precisely, they both are synchronous, two-sided modes of communication. This might cause delays when one of the processes has to wait frequently or longer for the other process. One-sided communication alleviates this issue by offering routines for remote memory access (RMA) that allow individual processes to initiate communication as either a sending or receiving party. RMA implements zero-copy networking by which data can be directly transferred between the main memories instead of passing it through the whole memory hierarchy up to the CPU. While the previously introduced two-sided communication routines require matching send and receive operations, where both participating processes have to anticpate the transfer, one-sided communication routes are more permissive. \n",
    "\n",
    "In general, one-sided communication operates according to the following pattern: \n",
    "* Allocate memory __windows__ through a collective routine\n",
    "* Start an RMA __epoch__\n",
    "* Communicate via put, get, and accumulate\n",
    "* Stop an RMA __epoch__\n",
    "* Deallocate memory windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f63b77-7a66-454f-b591-30bc44708468",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Windows\n",
    "\n",
    "In order to make one-sided communication possible, all processes must establish buffers in their respective local memory that remote processes can access at will. These buffers are called \"windows\" and they are created through a collective call that is executed by all processes willing to operate on these windows.\n",
    "\n",
    "Three routines are provided for window creation:\n",
    "* [MPI.Win.create](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html?#mpi4py.MPI.Win.Create), in case the memory buffer is already allocated,\n",
    "* [MPI.Win.allocate](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Allocate) and [MPI.Win.allocate_shared](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Allocate_shared), is case the memory has yet to be allocated, and \n",
    "* [MPI.Win.create_dynamic](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html?#mpi4py.MPI.Win.Create_dynamic), in case the necessary size of the memory buffer is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e349724-3463-48d0-a8a0-44610b837e25",
   "metadata": {},
   "source": [
    "#### Epochs & Synchronization\n",
    "\n",
    "The MPI standard defines two ways of accessing the window of a remote process: With __active target sycnhronization__, the remote window can only be accessed during a specific time period, the so-called \"epoch\", which is initiated by calling [MPI.Win.Fence](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Fence) by both the origin and the target process and also ended by calling it a second time by both processes. Between these two calles, the window can be accessed by a remote process. On the other hand, __passive target synchronization__ only requires the origin process to [MPI.Win.Lock](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Lock) and [MPI.Win.Unlock](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Unlock) the target window between performing any operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463addb3-38f1-4e7e-b362-e9dfbc88579a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Put, Get, Accumulate\n",
    "\n",
    "One-sided communication relies on three basic communication routines:\n",
    "* [MPI.Win.Get](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Get) for remote reads,\n",
    "* [MPI.Win.Put](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Put) for remote writes, and\n",
    "* [MPI.Win.Accumulate](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Win.html#mpi4py.MPI.Win.Accumulate) for remote updates where basic operations, such as summation or replacement, can be performed at the target windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c96ee-7914-42ec-b621-1dcd891adddf",
   "metadata": {},
   "source": [
    "#### Get with Active Target Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8fb82-0cfa-428d-a276-cebb8e7c584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_get.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "from mpi4py.util import dtlib\n",
    "\n",
    "\n",
    "mpi_dtype = MPI.INT\n",
    "np_dtype = dtlib.to_numpy_dtype(mpi_dtype)\n",
    "itemsize = mpi_dtype.Get_size()\n",
    "\n",
    "sndbuf = np.empty((), dtype=np_dtype)\n",
    "rcvbuf = np.empty_like(sndbuf)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "right = (rank+1) % size;\n",
    "left  = (rank-1+size) % size;\n",
    "sum = 0;\n",
    "np.copyto(sndbuf, rank)\n",
    "\n",
    "win = MPI.Win.Create(memory=sndbuf, disp_unit=sndbuf.itemsize, info=MPI.INFO_NULL, comm=comm) # create window\n",
    "\n",
    "for i in range(size):\n",
    "    win.Fence(MPI.MODE_NOPUT | MPI.MODE_NOPRECEDE) # active target synchronization\n",
    "    win.Get((rcvbuf, 1, MPI.INT), left, (0, 1, MPI.INT))\n",
    "    win.Fence(MPI.MODE_NOSTORE | MPI.MODE_NOPUT | MPI.MODE_NOSUCCEED)  # active target synchronization\n",
    "    np.copyto(sndbuf, rcvbuf)\n",
    "    sum += rcvbuf\n",
    "\n",
    "win.Free() # free window\n",
    "\n",
    "print(f'Process {rank} computes\\tsum = {sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0822c70-6b14-42e2-be1f-f26811893805",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 6 --oversubscribe python3 mpi4py/demo_osc-get-ats.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4dd595-6ab8-4437-823a-df26c066fa71",
   "metadata": {},
   "source": [
    "#### Get & Put with Passive Target Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcaf1c-9e8b-4216-b128-7de4afa90c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi4py/demo_osc-getput-pts.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "from mpi4py.util import dtlib\n",
    "\n",
    "\n",
    "mpi_dtype = MPI.INT\n",
    "np_dtype = dtlib.to_numpy_dtype(mpi_dtype)\n",
    "item_size = mpi_dtype.Get_size()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "comm_size = comm.Get_size()\n",
    "\n",
    "buf = None\n",
    "win_size = (comm_size * item_size) if rank == 0 else 0\n",
    "win = MPI.Win.Allocate(win_size, comm=comm)\n",
    "\n",
    "if rank == 0:\n",
    "    buf = np.arange(start=0, stop=comm_size, dtype=np_dtype)\n",
    "    win.Lock(rank=0)\n",
    "    win.Put(buf, target_rank=0)\n",
    "    win.Unlock(rank=0)\n",
    "    comm.Barrier()\n",
    "else:\n",
    "    buf = np.empty((comm_size), dtype=np_dtype)\n",
    "    comm.Barrier()\n",
    "    win.Lock(rank=0)\n",
    "    win.Get(buf, target_rank=0)\n",
    "    time.sleep(1)\n",
    "    win.Unlock(rank=0)\n",
    "\n",
    "win.Free() # free window\n",
    "    \n",
    "print(f'Process {rank} computes\\tbuf = {buf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029a9f1-859f-4367-ab99-4cd8d75baa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --np 6 --oversubscribe python3 mpi4py/demo_osc-getput-pts.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d2533a8417e93bb270061fddcf202607a399e1158d66c4e8746479fc11cda46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
